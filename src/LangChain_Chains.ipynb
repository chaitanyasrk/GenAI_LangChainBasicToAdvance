{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1P22T57tldy"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai python-dotenv\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4J9kecVtt7q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9DNSr6LtxyA",
        "outputId": "33a02d4a-8d8f-469e-ed44-ae415bbeabc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up Azure OpenAI credentials...\n",
            "You can find these in your Azure OpenAI resource in the Azure portal\n",
            "Enter your Azure OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Enter your Azure OpenAI Endpoint (e.g., https://your-resource.openai.azure.com/): https://chait-ma9dwmvx-eastus.openai.azure.com/\n",
            "Enter your GPT-4o deployment name (default: gpt4o): gpt-4o\n",
            "‚úÖ Credentials configured!\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß Setting up Azure OpenAI credentials...\")\n",
        "print(\"You can find these in your Azure OpenAI resource in the Azure portal\")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_API_KEY'):\n",
        "    os.environ['AZURE_OPENAI_API_KEY'] = getpass(\"Enter your Azure OpenAI API Key: \")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_ENDPOINT'):\n",
        "    os.environ['AZURE_OPENAI_ENDPOINT'] = input(\"Enter your Azure OpenAI Endpoint (e.g., https://your-resource.openai.azure.com/): \")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'):\n",
        "    deployment_name = input(\"Enter your GPT-4o deployment name (default: gpt4o): \") or \"gpt-4o\"\n",
        "    os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'] = deployment_name\n",
        "\n",
        "print(\"‚úÖ Credentials configured!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF2kEftfuxle"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjVZ2i1-vEEY",
        "outputId": "5f052f5f-c95f-47d3-97b2-534002b08002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Initializing Azure OpenAI connection...\n"
          ]
        }
      ],
      "source": [
        "print(\"ü§ñ Initializing Azure OpenAI connection...\")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "    temperature=0.7\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ELY1n-mvJql",
        "outputId": "1b67bd16-9dc6-4b55-a3ed-585f6fa1bef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing connection...\n",
            "‚úÖ Connection successful: Hello from Azure OpenAI! üòä\n"
          ]
        }
      ],
      "source": [
        "print(\"üß™ Testing connection...\")\n",
        "test_response = llm.invoke([HumanMessage(content=\"Say 'Hello from Azure OpenAI!'\")])\n",
        "print(f\"‚úÖ Connection successful: {test_response.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-lwBBTlwH71"
      },
      "outputs": [],
      "source": [
        "idea_template = \"\"\"\n",
        "You are a creative brainstorming assistant. Generate 5 innovative ideas for the following topic.\n",
        "Make each idea unique and practical.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Ideas:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6DAmuJvwMsX"
      },
      "outputs": [],
      "source": [
        "idea_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=idea_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5o6MwOWwWY_",
        "outputId": "a7f3874a-acef-4161-c340-0997a35e1d88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-819140694.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  idea_chain = LLMChain(\n"
          ]
        }
      ],
      "source": [
        "# Create the chain\n",
        "idea_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=idea_prompt,\n",
        "    verbose=True  # This shows the prompt being sent\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB26pKKrweUk",
        "outputId": "d73d3d25-1fcb-41ff-c66d-562b4d40a67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Testing your first chain...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are a creative brainstorming assistant. Generate 5 innovative ideas for the following topic.\n",
            "Make each idea unique and practical.\n",
            "\n",
            "Topic: sustainable energy solutions\n",
            "\n",
            "Ideas:\n",
            "\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-678142800.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = idea_chain.run(topic=\"sustainable energy solutions\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "==================================================\n",
            "RESULT:\n",
            "1. **Solar-Powered Modular Microgrids for Remote Communities**  \n",
            "   Develop portable, modular microgrid systems powered by solar panels that can be assembled like building blocks. These microgrids could provide sustainable energy to remote, off-grid areas or disaster-stricken regions. The modular design allows for scalability, so communities can expand their energy capacity as needed. The system would include integrated battery storage and could be connected to nearby grids once available.\n",
            "\n",
            "2. **Kinetic Energy Harvesting Sidewalks**  \n",
            "   Install piezoelectric tiles in high-foot-traffic urban areas like sidewalks, subway stations, and stadium entrances. These tiles convert the pressure and vibrations from footsteps into electricity. The collected energy could power streetlights, public Wi-Fi hotspots, or EV (electric vehicle) charging stations. Additionally, these tiles could be paired with an app that tracks energy generated by users, incentivizing walking with rewards like discounts on public transport.\n",
            "\n",
            "3. **Biodegradable Algae-Based Batteries**  \n",
            "   Create eco-friendly batteries using algae as the primary material for energy storage. Algae, being abundant and fast-growing, could serve as a renewable alternative to rare and environmentally damaging materials like lithium. These algae-based batteries could power low-energy devices, such as sensors in smart homes, and would break down naturally after their lifecycle, leaving no harmful waste behind.\n",
            "\n",
            "4. **Ocean Current Energy Farms**  \n",
            "   Design floating \"energy farms\" equipped with underwater turbines that harness the kinetic energy of ocean currents. These turbines would be tethered to the seafloor and positioned in regions with predictable currents, such as near coastal cities. The energy harvested could be transmitted via underwater cables to the mainland grid. The farms would be designed to minimize disruption to marine ecosystems by incorporating fish-friendly turbine designs.\n",
            "\n",
            "5. **Agrivoltaics with Smart Crop Sensors**  \n",
            "   Combine solar panel installations with agricultural land by using agrivoltaic systems, which place solar panels above crops. The shade provided by the panels can reduce water evaporation, while the energy generated is used to power smart sensors and irrigation systems. These sensors monitor soil moisture, weather conditions, and plant health in real time, optimizing water and fertilizer use. This dual-purpose solution maximizes land efficiency while reducing environmental impact.\n"
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Testing your first chain...\")\n",
        "result = idea_chain.run(topic=\"sustainable energy solutions\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULT:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2N9IJPbwpHm",
        "outputId": "5bc3680f-53fd-4146-8848-31bf798d1f82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèóÔ∏è Building sequential chain pipeline...\n"
          ]
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# ## 4. Sequential Chaining - The Main Event\n",
        "#\n",
        "# Now let's build our content creation pipeline with multiple connected chains.\n",
        "\n",
        "# %%\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "print(\"üèóÔ∏è Building sequential chain pipeline...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMGQM1WVwvSL"
      },
      "outputs": [],
      "source": [
        "# Chain 1: Topic to Outline\n",
        "outline_template = \"\"\"\n",
        "You are a content strategist. Create a detailed outline for the topic: {topic}\n",
        "Make it structured with main points and sub-points.\n",
        "Format as a clear, numbered outline.\n",
        "\n",
        "Topic: {topic}\n",
        "Outline:\"\"\"\n",
        "\n",
        "outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=outline_template\n",
        ")\n",
        "outline_chain = LLMChain(llm=llm, prompt=outline_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caWEDYxTw2zS"
      },
      "outputs": [],
      "source": [
        "# Chain 2: Outline to Content\n",
        "content_template = \"\"\"\n",
        "You are a skilled writer. Using this outline, write detailed, engaging content.\n",
        "Make it informative and well-structured with proper headings.\n",
        "Keep it concise but comprehensive.\n",
        "\n",
        "Outline: {outline}\n",
        "\n",
        "Content:\"\"\"\n",
        "\n",
        "content_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\"],\n",
        "    template=content_template\n",
        ")\n",
        "content_chain = LLMChain(llm=llm, prompt=content_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hh67NcMxBlK",
        "outputId": "a64c8db4-6258-4766-8a02-8b8c9e5dfdaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Individual chains created!\n"
          ]
        }
      ],
      "source": [
        "# Chain 3: Content to Summary\n",
        "summary_template = \"\"\"\n",
        "You are an editor. Create a concise summary of this content.\n",
        "Focus on key takeaways and main points. Keep it under 200 words.\n",
        "\n",
        "Content: {content}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=summary_template\n",
        ")\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, verbose=True)\n",
        "\n",
        "print(\"‚úÖ Individual chains created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9TrKZClxODX"
      },
      "source": [
        "# %% [markdown]\n",
        "# ## 5. Connecting the Chains\n",
        "#\n",
        "# Now let's connect all chains into a sequential pipeline.\n",
        "\n",
        "# %%\n",
        "# Create the sequential chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2CuitM8xGJT",
        "outputId": "d5902c7c-9be9-4729-cc5e-823123787781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó Sequential chain created!\n",
            "Pipeline: Topic ‚Üí Outline ‚Üí Content ‚Üí Summary\n"
          ]
        }
      ],
      "source": [
        "content_pipeline = SimpleSequentialChain(\n",
        "    chains=[outline_chain, content_chain, summary_chain],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"üîó Sequential chain created!\")\n",
        "print(\"Pipeline: Topic ‚Üí Outline ‚Üí Content ‚Üí Summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_BCs2QXxxIq"
      },
      "source": [
        "# %% [markdown]\n",
        "# ## 6. Run the Complete Pipeline\n",
        "#\n",
        "# Let's test our content creation pipeline with different topics.\n",
        "\n",
        "# %%\n",
        "# Test topics - feel free to modify these!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnUpIt2cxyQx"
      },
      "outputs": [],
      "source": [
        "test_topics = [\n",
        "    \"Machine Learning for Beginners\",\n",
        "    \"Microservices Architecture Best Practices\",\n",
        "    \"Azure Kubernetes Service (AKS) Setup\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS4CWvZPx-C_"
      },
      "outputs": [],
      "source": [
        "def run_content_pipeline(topic):\n",
        "    \"\"\"Run the content pipeline and display results nicely\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéØ TOPIC: {topic}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        result = content_pipeline.run(topic)\n",
        "        print(f\"\\n‚úÖ FINAL SUMMARY:\\n{result}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run pipeline for each test topic\n",
        "for topic in test_topics:\n",
        "    run_content_pipeline(topic)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPzPdrnczWcF"
      },
      "source": [
        "# ## 7. Advanced: Multi-Input Sequential Chain\n",
        "#\n",
        "# Let's create a more sophisticated chain that handles multiple inputs and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ef623b8"
      },
      "source": [
        "# LangChain Sequential Chaining with Azure OpenAI\n",
        "\n",
        "This notebook demonstrates how to build content creation pipelines using LangChain's sequential chaining with Azure OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fafcfcd6"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "First, let's install the necessary libraries and import the required modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "471f119e"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai python-dotenv\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98ba634c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50e6b15d"
      },
      "source": [
        "## 2. Configure Azure OpenAI Credentials\n",
        "\n",
        "Enter your Azure OpenAI API key, endpoint, and deployment name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0a5ed33"
      },
      "outputs": [],
      "source": [
        "print(\"üîß Setting up Azure OpenAI credentials...\")\n",
        "print(\"You can find these in your Azure OpenAI resource in the Azure portal\")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_API_KEY'):\n",
        "    os.environ['AZURE_OPENAI_API_KEY'] = getpass(\"Enter your Azure OpenAI API Key: \")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_ENDPOINT'):\n",
        "    os.environ['AZURE_OPENAI_ENDPOINT'] = input(\"Enter your Azure OpenAI Endpoint (e.g., https://your-resource.openai.azure.com/): \")\n",
        "\n",
        "if not os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'):\n",
        "    deployment_name = input(\"Enter your GPT-4o deployment name (default: gpt4o): \") or \"gpt-4o\"\n",
        "    os.environ['AZURE_OPENAI_DEPLOYMENT_NAME'] = deployment_name\n",
        "\n",
        "print(\"‚úÖ Credentials configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4173d418"
      },
      "source": [
        "## 3. Initialize and Test Azure OpenAI Connection\n",
        "\n",
        "Initialize the AzureChatOpenAI model and test the connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93732562"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7331a3ad"
      },
      "outputs": [],
      "source": [
        "print(\"ü§ñ Initializing Azure OpenAI connection...\")\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    deployment_name=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea64ced2"
      },
      "outputs": [],
      "source": [
        "print(\"üß™ Testing connection...\")\n",
        "test_response = llm.invoke([HumanMessage(content=\"Say 'Hello from Azure OpenAI!'\")])\n",
        "print(f\"‚úÖ Connection successful: {test_response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3458fd72"
      },
      "source": [
        "## 4. Basic Single Chain Example\n",
        "\n",
        "Let's create a simple chain to brainstorm ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af6005f3"
      },
      "outputs": [],
      "source": [
        "idea_template = \"\"\"\n",
        "You are a creative brainstorming assistant. Generate 5 innovative ideas for the following topic.\n",
        "Make each idea unique and practical.\n",
        "\n",
        "Topic: {topic}\n",
        "\n",
        "Ideas:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bcabd89"
      },
      "outputs": [],
      "source": [
        "idea_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=idea_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f21f023"
      },
      "outputs": [],
      "source": [
        "# Create the chain\n",
        "idea_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=idea_prompt,\n",
        "    verbose=True  # This shows the prompt being sent\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a41aecd"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Testing your first chain...\")\n",
        "result = idea_chain.run(topic=\"sustainable energy solutions\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULT:\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b506f87"
      },
      "source": [
        "## 5. Sequential Chaining - The Main Event\n",
        "\n",
        "Now let's build our content creation pipeline with multiple connected chains using `SimpleSequentialChain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a804900b"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "print(\"üèóÔ∏è Building sequential chain pipeline...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7385450"
      },
      "outputs": [],
      "source": [
        "# Chain 1: Topic to Outline\n",
        "outline_template = \"\"\"\n",
        "You are a content strategist. Create a detailed outline for the topic: {topic}\n",
        "Make it structured with main points and sub-points.\n",
        "Format as a clear, numbered outline.\n",
        "\n",
        "Topic: {topic}\n",
        "Outline:\"\"\"\n",
        "\n",
        "outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=outline_template\n",
        ")\n",
        "outline_chain = LLMChain(llm=llm, prompt=outline_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d9f3898"
      },
      "outputs": [],
      "source": [
        "# Chain 2: Outline to Content\n",
        "content_template = \"\"\"\n",
        "You are a skilled writer. Using this outline, write detailed, engaging content.\n",
        "Make it informative and well-structured with proper headings.\n",
        "Keep it concise but comprehensive.\n",
        "\n",
        "Outline: {outline}\n",
        "\n",
        "Content:\"\"\"\n",
        "\n",
        "content_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\"],\n",
        "    template=content_template\n",
        ")\n",
        "content_chain = LLMChain(llm=llm, prompt=content_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4960f19"
      },
      "outputs": [],
      "source": [
        "# Chain 3: Content to Summary\n",
        "summary_template = \"\"\"\n",
        "You are an editor. Create a concise summary of this content.\n",
        "Focus on key takeaways and main points. Keep it under 200 words.\n",
        "\n",
        "Content: {content}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    template=summary_template\n",
        ")\n",
        "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, verbose=True)\n",
        "\n",
        "print(\"‚úÖ Individual chains created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e547eb5a"
      },
      "source": [
        "## 6. Connecting the Chains\n",
        "\n",
        "Now let's connect all chains into a sequential pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afe25e4b"
      },
      "outputs": [],
      "source": [
        "# Create the sequential chain\n",
        "content_pipeline = SimpleSequentialChain(\n",
        "    chains=[outline_chain, content_chain, summary_chain],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"üîó Sequential chain created!\")\n",
        "print(\"Pipeline: Topic ‚Üí Outline ‚Üí Content ‚Üí Summary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d36c27"
      },
      "source": [
        "## 7. Run the Complete Pipeline\n",
        "\n",
        "Let's test our content creation pipeline with different topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4cba6ad"
      },
      "outputs": [],
      "source": [
        "# Test topics - feel free to modify these!\n",
        "test_topics = [\n",
        "    \"Machine Learning for Beginners\",\n",
        "    \"Microservices Architecture Best Practices\",\n",
        "    \"Azure Kubernetes Service (AKS) Setup\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7efc79a2"
      },
      "outputs": [],
      "source": [
        "def run_content_pipeline(topic):\n",
        "    \"\"\"Run the content pipeline and display results nicely\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéØ TOPIC: {topic}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        result = content_pipeline.run(topic)\n",
        "        print(f\"\\n‚úÖ FINAL SUMMARY:\\n{result}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run pipeline for each test topic\n",
        "for topic in test_topics:\n",
        "    run_content_pipeline(topic)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de13ff4c"
      },
      "source": [
        "## 8. Advanced: Multi-Input Sequential Chain\n",
        "\n",
        "Let's create a more sophisticated chain that handles multiple inputs and outputs using `SequentialChain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d57ec18"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "print(\"üß™ Creating advanced multi-input sequential chain...\")\n",
        "\n",
        "# Enhanced outline chain with audience consideration\n",
        "advanced_outline_template = \"\"\"\n",
        "You are a content strategist. Create a detailed outline for the topic: {topic}\n",
        "Target audience: {audience}\n",
        "Writing tone: {tone}\n",
        "\n",
        "Make it structured and appropriate for your audience.\n",
        "\n",
        "Topic: {topic}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Outline:\"\"\"\n",
        "\n",
        "advanced_outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"audience\", \"tone\"],\n",
        "    template=advanced_outline_template\n",
        ")\n",
        "advanced_outline_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_outline_prompt,\n",
        "    output_key=\"outline\"\n",
        ")\n",
        "\n",
        "# Enhanced content chain\n",
        "advanced_content_template = \"\"\"\n",
        "Write detailed content based on this outline.\n",
        "Target audience: {audience}\n",
        "Writing tone: {tone}\n",
        "\n",
        "Tailor your language and examples to the audience.\n",
        "\n",
        "Outline: {outline}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Content:\"\"\"\n",
        "\n",
        "advanced_content_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\", \"audience\", \"tone\"],\n",
        "    template=advanced_content_template\n",
        ")\n",
        "advanced_content_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_content_prompt,\n",
        "    output_key=\"content\"\n",
        ")\n",
        "\n",
        "# Enhanced summary chain\n",
        "advanced_summary_template = \"\"\"\n",
        "Create a summary tailored to: {audience}\n",
        "Use a {tone} tone.\n",
        "\n",
        "Content: {content}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "advanced_summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\", \"audience\", \"tone\"],\n",
        "    template=advanced_summary_template\n",
        ")\n",
        "advanced_summary_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_summary_prompt,\n",
        "    output_key=\"summary\"\n",
        ")\n",
        "\n",
        "# Create the advanced sequential chain\n",
        "advanced_pipeline = SequentialChain(\n",
        "    chains=[advanced_outline_chain, advanced_content_chain, advanced_summary_chain],\n",
        "    input_variables=[\"topic\", \"audience\", \"tone\"],\n",
        "    output_variables=[\"outline\", \"content\", \"summary\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Advanced sequential chain created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b56f8375"
      },
      "source": [
        "## 9. Test the Advanced Pipeline\n",
        "\n",
        "Run the advanced pipeline with different inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "712caaad"
      },
      "outputs": [],
      "source": [
        "def run_advanced_pipeline(topic, audience, tone):\n",
        "    \"\"\"Run the advanced pipeline with multiple inputs\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéØ TOPIC: {topic}\")\n",
        "    print(f\"üë• AUDIENCE: {audience}\")\n",
        "    print(f\"üé≠ TONE: {tone}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        result = advanced_pipeline({\n",
        "            \"topic\": topic,\n",
        "            \"audience\": audience,\n",
        "            \"tone\": tone\n",
        "        })\n",
        "\n",
        "        print(f\"\\nüìã OUTLINE:\\n{result['outline'][:200]}...\")\n",
        "        print(f\"\\nüìù CONTENT:\\n{result['content'][:300]}...\")\n",
        "        print(f\"\\n‚úÖ SUMMARY:\\n{result['summary']}\")\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test different combinations\n",
        "test_cases = [\n",
        "    {\n",
        "        \"topic\": \"Kubernetes for Production\",\n",
        "        \"audience\": \"DevOps engineers\",\n",
        "        \"tone\": \"technical and detailed\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"Cloud Computing Basics\",\n",
        "        \"audience\": \"business executives\",\n",
        "        \"tone\": \"friendly and non-technical\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"API Design Patterns\",\n",
        "        \"audience\": \"junior developers\",\n",
        "        \"tone\": \"educational and encouraging\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for test_case in test_cases:\n",
        "    run_advanced_pipeline(**test_case)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fc5babc"
      },
      "source": [
        "## 10. Error Handling and Debugging\n",
        "\n",
        "Let's implement robust error handling for production use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c70e61a"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RobustContentPipeline:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.setup_chains()\n",
        "\n",
        "    def setup_chains(self):\n",
        "        \"\"\"Initialize all chains with error handling\"\"\"\n",
        "        try:\n",
        "            # Outline chain\n",
        "            self.outline_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"topic\"],\n",
        "                    template=\"Create a brief outline for: {topic}\\nOutline:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Content chain\n",
        "            self.content_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"outline\"],\n",
        "                    template=\"Write content based on: {outline}\\nContent:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Summary chain\n",
        "            self.summary_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"content\"],\n",
        "                    template=\"Summarize: {content}\\nSummary:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            logger.info(\"‚úÖ All chains initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to initialize chains: {e}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, topic: str) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete pipeline with error handling\"\"\"\n",
        "        results = {\"topic\": topic}\n",
        "\n",
        "        try:\n",
        "            # Step 1: Generate outline\n",
        "            logger.info(\"üîÑ Generating outline...\")\n",
        "            outline = self.outline_chain.run(topic=topic)\n",
        "            results[\"outline\"] = outline\n",
        "            logger.info(\"‚úÖ Outline generated\")\n",
        "\n",
        "            # Step 2: Generate content\n",
        "            logger.info(\"üîÑ Generating content...\")\n",
        "            content = self.content_chain.run(outline=outline)\n",
        "            results[\"content\"] = content\n",
        "            logger.info(\"‚úÖ Content generated\")\n",
        "\n",
        "            # Step 3: Generate summary\n",
        "            logger.info(\"üîÑ Generating summary...\")\n",
        "            summary = self.summary_chain.run(content=content)\n",
        "            results[\"summary\"] = summary\n",
        "            logger.info(\"‚úÖ Summary generated\")\n",
        "\n",
        "            results[\"status\"] = \"success\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Pipeline failed at step: {e}\")\n",
        "            results[\"status\"] = \"failed\"\n",
        "            results[\"error\"] = str(e)\n",
        "\n",
        "        return results\n",
        "\n",
        "# Test the robust pipeline\n",
        "print(\"üõ°Ô∏è Testing robust pipeline with error handling...\")\n",
        "\n",
        "robust_pipeline = RobustContentPipeline(llm)\n",
        "result = robust_pipeline.run_pipeline(\"Azure Container Instances vs Azure Kubernetes Service\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ROBUST PIPELINE RESULT:\")\n",
        "print(f\"Status: {result['status']}\")\n",
        "if result['status'] == 'success':\n",
        "    print(f\"Summary: {result['summary']}\")\n",
        "else:\n",
        "    print(f\"Error: {result.get('error', 'Unknown error')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2540a32"
      },
      "source": [
        "## 11. Interactive Testing Section\n",
        "\n",
        "Now you can test with your own topics and configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eb0ed72"
      },
      "outputs": [],
      "source": [
        "# Interactive testing function\n",
        "def interactive_test():\n",
        "    \"\"\"Interactive function for testing different topics\"\"\"\n",
        "    print(\"üéÆ Interactive Testing Mode\")\n",
        "    print(\"Enter 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        topic = input(\"\\nEnter a topic to generate content for: \")\n",
        "        if topic.lower() == 'quit':\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if topic.strip():\n",
        "            print(f\"\\nüöÄ Processing: {topic}\")\n",
        "            result = robust_pipeline.run_pipeline(topic)\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                print(f\"\\n‚úÖ SUMMARY:\\n{result['summary']}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå ERROR: {result.get('error', 'Unknown error')}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Please enter a valid topic\")\n",
        "\n",
        "# Uncomment the line below to start interactive mode\n",
        "# interactive_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "defaee30"
      },
      "source": [
        "## 12. Performance Monitoring\n",
        "\n",
        "Let's add some basic performance monitoring to understand execution times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6904fcaa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import List\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = []\n",
        "\n",
        "    def time_execution(self, func, *args, **kwargs):\n",
        "        \"\"\"Time the execution of a function\"\"\"\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "\n",
        "        execution_time = end_time - start_time\n",
        "        self.metrics.append({\n",
        "            'function': func.__name__,\n",
        "            'execution_time': execution_time,\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        })\n",
        "\n",
        "        return result, execution_time\n",
        "\n",
        "    def get_average_time(self) -> float:\n",
        "        \"\"\"Get average execution time\"\"\"\n",
        "        if not self.metrics:\n",
        "            return 0\n",
        "        return sum(m['execution_time'] for m in self.metrics) / len(self.metrics)\n",
        "\n",
        "    def print_stats(self):\n",
        "        \"\"\"Print performance statistics\"\"\"\n",
        "        if not self.metrics:\n",
        "            print(\"üìä No metrics available\")\n",
        "            return\n",
        "\n",
        "        print(\"üìä PERFORMANCE STATS:\")\n",
        "        print(f\"Total executions: {len(self.metrics)}\")\n",
        "        print(f\"Average time: {self.get_average_time():.2f} seconds\")\n",
        "        print(f\"Fastest: {min(m['execution_time'] for m in self.metrics):.2f}s\")\n",
        "        print(f\"Slowest: {max(m['execution_time'] for m in self.metrics):.2f}s\")\n",
        "\n",
        "# Test performance monitoring\n",
        "monitor = PerformanceMonitor()\n",
        "\n",
        "def timed_pipeline_run(topic):\n",
        "    \"\"\"Wrapper function for timing pipeline execution\"\"\"\n",
        "    return robust_pipeline.run_pipeline(topic)\n",
        "\n",
        "# Run multiple tests to gather performance data\n",
        "test_topics = [\n",
        "    \"Docker containerization best practices\",\n",
        "    \"RESTful API design principles\",\n",
        "    \"Database indexing strategies\"\n",
        "]\n",
        "\n",
        "print(\"‚è±Ô∏è Running performance tests...\")\n",
        "\n",
        "for topic in test_topics:\n",
        "    print(f\"\\nüîÑ Testing: {topic}\")\n",
        "    result, exec_time = monitor.time_execution(timed_pipeline_run, topic)\n",
        "    print(f\"‚è∞ Execution time: {exec_time:.2f} seconds\")\n",
        "\n",
        "# Display performance statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "monitor.print_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0b72871"
      },
      "source": [
        "## 13. Key Takeaways and Next Steps\n",
        "\n",
        "Congratulations! You've learned the core concepts of LangChain sequential chaining.\n",
        "\n",
        "### What You've Accomplished:\n",
        "- ‚úÖ Set up Azure OpenAI with LangChain\n",
        "- ‚úÖ Created basic and advanced sequential chains\n",
        "- ‚úÖ Implemented error handling and monitoring\n",
        "- ‚úÖ Built a production-ready content pipeline\n",
        "\n",
        "### Next Steps to Explore:\n",
        "1. **Memory Integration** - Add conversation history to chains\n",
        "2. **Router Chains** - Conditional logic based on inputs\n",
        "3. **Map-Reduce Chains** - Process large datasets in parallel\n",
        "4. **Custom Chains** - Build domain-specific logic\n",
        "5. **Agent Frameworks** - Let LLMs use tools and make decisions\n",
        "\n",
        "### Integration with Your Stack:\n",
        "- Deploy chains as **Azure Functions**\n",
        "- Use with **Azure Container Instances** or **AKS**\n",
        "- Integrate with your **.NET Core APIs**\n",
        "- Monitor with **Application Insights** and **Grafana**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cf4d141"
      },
      "outputs": [],
      "source": [
        "print(\"üéâ Tutorial Complete!\")\n",
        "print(\"\\nYou now have a solid foundation in LangChain sequential chaining.\")\n",
        "print(\"Feel free to modify the code above and experiment with different:\")\n",
        "print(\"- Topics and prompts\")\n",
        "print(\"- Chain configurations\")\n",
        "print(\"- Error handling strategies\")\n",
        "print(\"- Performance optimizations\")\n",
        "print(\"\\nHappy coding! üöÄ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e1aebca"
      },
      "source": [
        "## Bonus: Quick Reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqCin4jJzbON"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "print(\"üß™ Creating advanced multi-input sequential chain...\")\n",
        "\n",
        "# Enhanced outline chain with audience consideration\n",
        "advanced_outline_template = \"\"\"\n",
        "You are a content strategist. Create a detailed outline for the topic: {topic}\n",
        "Target audience: {audience}\n",
        "Writing tone: {tone}\n",
        "\n",
        "Make it structured and appropriate for your audience.\n",
        "\n",
        "Topic: {topic}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Outline:\"\"\"\n",
        "\n",
        "advanced_outline_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"audience\", \"tone\"],\n",
        "    template=advanced_outline_template\n",
        ")\n",
        "advanced_outline_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_outline_prompt,\n",
        "    output_key=\"outline\"\n",
        ")\n",
        "\n",
        "# Enhanced content chain\n",
        "advanced_content_template = \"\"\"\n",
        "Write detailed content based on this outline.\n",
        "Target audience: {audience}\n",
        "Writing tone: {tone}\n",
        "\n",
        "Tailor your language and examples to the audience.\n",
        "\n",
        "Outline: {outline}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Content:\"\"\"\n",
        "\n",
        "advanced_content_prompt = PromptTemplate(\n",
        "    input_variables=[\"outline\", \"audience\", \"tone\"],\n",
        "    template=advanced_content_template\n",
        ")\n",
        "advanced_content_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_content_prompt,\n",
        "    output_key=\"content\"\n",
        ")\n",
        "\n",
        "# Enhanced summary chain\n",
        "advanced_summary_template = \"\"\"\n",
        "Create a summary tailored to: {audience}\n",
        "Use a {tone} tone.\n",
        "\n",
        "Content: {content}\n",
        "Audience: {audience}\n",
        "Tone: {tone}\n",
        "\n",
        "Summary:\"\"\"\n",
        "\n",
        "advanced_summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"content\", \"audience\", \"tone\"],\n",
        "    template=advanced_summary_template\n",
        ")\n",
        "advanced_summary_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=advanced_summary_prompt,\n",
        "    output_key=\"summary\"\n",
        ")\n",
        "\n",
        "# Create the advanced sequential chain\n",
        "advanced_pipeline = SequentialChain(\n",
        "    chains=[advanced_outline_chain, advanced_content_chain, advanced_summary_chain],\n",
        "    input_variables=[\"topic\", \"audience\", \"tone\"],\n",
        "    output_variables=[\"outline\", \"content\", \"summary\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Advanced sequential chain created!\")\n",
        "\n",
        "\n",
        "# ## 8. Test the Advanced Pipeline\n",
        "\n",
        "def run_advanced_pipeline(topic, audience, tone):\n",
        "    \"\"\"Run the advanced pipeline with multiple inputs\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üéØ TOPIC: {topic}\")\n",
        "    print(f\"üë• AUDIENCE: {audience}\")\n",
        "    print(f\"üé≠ TONE: {tone}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        result = advanced_pipeline({\n",
        "            \"topic\": topic,\n",
        "            \"audience\": audience,\n",
        "            \"tone\": tone\n",
        "        })\n",
        "\n",
        "        print(f\"\\nüìã OUTLINE:\\n{result['outline'][:200]}...\")\n",
        "        print(f\"\\nüìù CONTENT:\\n{result['content'][:300]}...\")\n",
        "        print(f\"\\n‚úÖ SUMMARY:\\n{result['summary']}\")\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test different combinations\n",
        "test_cases = [\n",
        "    {\n",
        "        \"topic\": \"Kubernetes for Production\",\n",
        "        \"audience\": \"DevOps engineers\",\n",
        "        \"tone\": \"technical and detailed\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"Cloud Computing Basics\",\n",
        "        \"audience\": \"business executives\",\n",
        "        \"tone\": \"friendly and non-technical\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"API Design Patterns\",\n",
        "        \"audience\": \"junior developers\",\n",
        "        \"tone\": \"educational and encouraging\"\n",
        "    }\n",
        "]\n",
        "\n",
        "for test_case in test_cases:\n",
        "    run_advanced_pipeline(**test_case)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "\n",
        "#\n",
        "# ## 9. Error Handling and Debugging\n",
        "#\n",
        "# Let's implement robust error handling for production use.\n",
        "\n",
        "#\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class RobustContentPipeline:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.setup_chains()\n",
        "\n",
        "    def setup_chains(self):\n",
        "        \"\"\"Initialize all chains with error handling\"\"\"\n",
        "        try:\n",
        "            # Outline chain\n",
        "            self.outline_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"topic\"],\n",
        "                    template=\"Create a brief outline for: {topic}\\nOutline:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Content chain\n",
        "            self.content_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"outline\"],\n",
        "                    template=\"Write content based on: {outline}\\nContent:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Summary chain\n",
        "            self.summary_chain = LLMChain(\n",
        "                llm=self.llm,\n",
        "                prompt=PromptTemplate(\n",
        "                    input_variables=[\"content\"],\n",
        "                    template=\"Summarize: {content}\\nSummary:\"\n",
        "                )\n",
        "            )\n",
        "\n",
        "            logger.info(\"‚úÖ All chains initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to initialize chains: {e}\")\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self, topic: str) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete pipeline with error handling\"\"\"\n",
        "        results = {\"topic\": topic}\n",
        "\n",
        "        try:\n",
        "            # Step 1: Generate outline\n",
        "            logger.info(\"üîÑ Generating outline...\")\n",
        "            outline = self.outline_chain.run(topic=topic)\n",
        "            results[\"outline\"] = outline\n",
        "            logger.info(\"‚úÖ Outline generated\")\n",
        "\n",
        "            # Step 2: Generate content\n",
        "            logger.info(\"üîÑ Generating content...\")\n",
        "            content = self.content_chain.run(outline=outline)\n",
        "            results[\"content\"] = content\n",
        "            logger.info(\"‚úÖ Content generated\")\n",
        "\n",
        "            # Step 3: Generate summary\n",
        "            logger.info(\"üîÑ Generating summary...\")\n",
        "            summary = self.summary_chain.run(content=content)\n",
        "            results[\"summary\"] = summary\n",
        "            logger.info(\"‚úÖ Summary generated\")\n",
        "\n",
        "            results[\"status\"] = \"success\"\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Pipeline failed at step: {e}\")\n",
        "            results[\"status\"] = \"failed\"\n",
        "            results[\"error\"] = str(e)\n",
        "\n",
        "        return results\n",
        "\n",
        "# Test the robust pipeline\n",
        "print(\"üõ°Ô∏è Testing robust pipeline with error handling...\")\n",
        "\n",
        "robust_pipeline = RobustContentPipeline(llm)\n",
        "result = robust_pipeline.run_pipeline(\"Azure Container Instances vs Azure Kubernetes Service\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ROBUST PIPELINE RESULT:\")\n",
        "print(f\"Status: {result['status']}\")\n",
        "if result['status'] == 'success':\n",
        "    print(f\"Summary: {result['summary']}\")\n",
        "else:\n",
        "    print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "#\n",
        "# ## 10. Interactive Testing Section\n",
        "#\n",
        "# Now you can test with your own topics and configurations!\n",
        "\n",
        "# %%\n",
        "# Interactive testing function\n",
        "def interactive_test():\n",
        "    \"\"\"Interactive function for testing different topics\"\"\"\n",
        "    print(\"üéÆ Interactive Testing Mode\")\n",
        "    print(\"Enter 'quit' to exit\")\n",
        "\n",
        "    while True:\n",
        "        topic = input(\"\\nEnter a topic to generate content for: \")\n",
        "        if topic.lower() == 'quit':\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if topic.strip():\n",
        "            print(f\"\\nüöÄ Processing: {topic}\")\n",
        "            result = robust_pipeline.run_pipeline(topic)\n",
        "\n",
        "            if result['status'] == 'success':\n",
        "                print(f\"\\n‚úÖ SUMMARY:\\n{result['summary']}\")\n",
        "            else:\n",
        "                print(f\"\\n‚ùå ERROR: {result.get('error', 'Unknown error')}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Please enter a valid topic\")\n",
        "\n",
        "# Uncomment the line below to start interactive mode\n",
        "# interactive_test()\n",
        "\n",
        "#\n",
        "# ## 11. Performance Monitoring\n",
        "#\n",
        "# Let's add some basic performance monitoring to understand execution times.\n",
        "\n",
        "# %%\n",
        "import time\n",
        "from typing import List\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    def __init__(self):\n",
        "        self.metrics = []\n",
        "\n",
        "    def time_execution(self, func, *args, **kwargs):\n",
        "        \"\"\"Time the execution of a function\"\"\"\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "\n",
        "        execution_time = end_time - start_time\n",
        "        self.metrics.append({\n",
        "            'function': func.__name__,\n",
        "            'execution_time': execution_time,\n",
        "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "        })\n",
        "\n",
        "        return result, execution_time\n",
        "\n",
        "    def get_average_time(self) -> float:\n",
        "        \"\"\"Get average execution time\"\"\"\n",
        "        if not self.metrics:\n",
        "            return 0\n",
        "        return sum(m['execution_time'] for m in self.metrics) / len(self.metrics)\n",
        "\n",
        "    def print_stats(self):\n",
        "        \"\"\"Print performance statistics\"\"\"\n",
        "        if not self.metrics:\n",
        "            print(\"üìä No metrics available\")\n",
        "            return\n",
        "\n",
        "        print(\"üìä PERFORMANCE STATS:\")\n",
        "        print(f\"Total executions: {len(self.metrics)}\")\n",
        "        print(f\"Average time: {self.get_average_time():.2f} seconds\")\n",
        "        print(f\"Fastest: {min(m['execution_time'] for m in self.metrics):.2f}s\")\n",
        "        print(f\"Slowest: {max(m['execution_time'] for m in self.metrics):.2f}s\")\n",
        "\n",
        "# Test performance monitoring\n",
        "monitor = PerformanceMonitor()\n",
        "\n",
        "def timed_pipeline_run(topic):\n",
        "    \"\"\"Wrapper function for timing pipeline execution\"\"\"\n",
        "    return robust_pipeline.run_pipeline(topic)\n",
        "\n",
        "# Run multiple tests to gather performance data\n",
        "test_topics = [\n",
        "    \"Docker containerization best practices\",\n",
        "    \"RESTful API design principles\",\n",
        "    \"Database indexing strategies\"\n",
        "]\n",
        "\n",
        "print(\"‚è±Ô∏è Running performance tests...\")\n",
        "\n",
        "for topic in test_topics:\n",
        "    print(f\"\\nüîÑ Testing: {topic}\")\n",
        "    result, exec_time = monitor.time_execution(timed_pipeline_run, topic)\n",
        "    print(f\"‚è∞ Execution time: {exec_time:.2f} seconds\")\n",
        "\n",
        "# Display performance statistics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "monitor.print_stats()\n",
        "\n",
        "#\n",
        "# ## 12. Key Takeaways and Next Steps\n",
        "#\n",
        "# Congratulations! You've learned the core concepts of LangChain sequential chaining.\n",
        "#\n",
        "# ### What You've Accomplished:\n",
        "# - ‚úÖ Set up Azure OpenAI with LangChain\n",
        "# - ‚úÖ Created basic and advanced sequential chains\n",
        "# - ‚úÖ Implemented error handling and monitoring\n",
        "# - ‚úÖ Built a production-ready content pipeline\n",
        "#\n",
        "# ### Next Steps to Explore:\n",
        "# 1. **Memory Integration** - Add conversation history to chains\n",
        "# 2. **Router Chains** - Conditional logic based on inputs\n",
        "# 3. **Map-Reduce Chains** - Process large datasets in parallel\n",
        "# 4. **Custom Chains** - Build domain-specific logic\n",
        "# 5. **Agent Frameworks** - Let LLMs use tools and make decisions\n",
        "#\n",
        "# ### Integration with Your Stack:\n",
        "# - Deploy chains as **Azure Functions**\n",
        "# - Use with **Azure Container Instances** or **AKS**\n",
        "# - Integrate with your **.NET Core APIs**\n",
        "# - Monitor with **Application Insights** and **Grafana**\n",
        "\n",
        "# %%\n",
        "print(\"üéâ Tutorial Complete!\")\n",
        "print(\"\\nYou now have a solid foundation in LangChain sequential chaining.\")\n",
        "print(\"Feel free to modify the code above and experiment with different:\")\n",
        "print(\"- Topics and prompts\")\n",
        "print(\"- Chain configurations\")\n",
        "print(\"- Error handling strategies\")\n",
        "print(\"- Performance optimizations\")\n",
        "print(\"\\nHappy coding! üöÄ\")\n",
        "\n",
        "#\n",
        "# ## Bonus: Quick Reference\n",
        "#\n",
        "# ```python\n",
        "# # Basic chain pattern\n",
        "# chain = LLMChain(llm=llm, prompt=prompt)\n",
        "# result = chain.run(input_variable=\"value\")\n",
        "#\n",
        "# # Sequential chain pattern\n",
        "# sequential_chain = SimpleSequentialChain(\n",
        "#     chains=[chain1, chain2, chain3],\n",
        "#     verbose=True\n",
        "# )\n",
        "# result = sequential_chain.run(\"input\")\n",
        "#\n",
        "# # Multi-input sequential chain\n",
        "# advanced_chain = SequentialChain(\n",
        "#     chains=[chain1, chain2],\n",
        "#     input_variables=[\"input1\", \"input2\"],\n",
        "#     output_variables=[\"output1\", \"output2\"]\n",
        "# )\n",
        "# result = advanced_chain({\"input1\": \"value1\", \"input2\": \"value2\"})\n",
        "# ```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
